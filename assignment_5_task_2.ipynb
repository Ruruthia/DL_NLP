{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 2 (6 points)\n",
    "\n",
    "This task is about text generation. You have to:\n",
    "\n",
    "**A**. Create text corpora containing texts with similar vocabulary (for instance books from the same genre, or written by the same author). This corpora should have approximately 1M words. You can consider using the following sources: Project Gutenberg (https://www.gutenberg.org/), Wolne Lektury (https://wolnelektury.pl/), parts of BookCorpus, https://github.com/soskek/bookcorpus, but generally feel free. Texts could be in English, Polish or any other language you know.\n",
    "\n",
    "**B**. choose the tokenization procedure. It should have two stages:\n",
    "\n",
    "1. word tokenization (you can use nltk.tokenize.word_tokenize, tokenizer from spaCy, pytorch, keras, ...). Test your tokenizer on your corpora, and look at a set of tokens containing both letters and special characters. If some of them should be in your opinion treated as a sequence of tokens, then modify the tokenization procedure\n",
    "\n",
    "2. sub-word tokenization (you can either use the existing procedure, like wordpiece or sentencepiece, or create something by yourself). Here is a simple idea: take 8K most popular words (W), 1K most popular suffixes (S), and 1K most popular prefixes (P). Words in W are its own tokens. Word x outside W should be tokenized as 'p_ _s' where p is the longest prefix of x in P, and s is the longest prefix of W\n",
    "\n",
    "**C**. write text generation procedure. The procedure should fulfill the following requirements:\n",
    "\n",
    "1. it should use the RNN language model (trained on sub-word tokens)\n",
    "2. generated tokens should be presented as a text containing words (without extra spaces, or other extra characters, as begin-of-word introduced during tokenization)\n",
    "3. all words in a generated text should belond to the corpora (note that this is not guaranteed by LSTM)\n",
    "4. in generation Top-P sampling should be used (see NN-NLP.6, slide X)\n",
    "5. in generated texts every token 3-gram should be uniq\n",
    "6. *(optionally, +1 point)* all token bigrams in generated texts occur in the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import re\n",
    "from io import open\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEQUENCE_LENGTH = 15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    data = open(path).read()\n",
    "    # Cutting off gutenberg credits\n",
    "    tokenized_data = word_tokenize(data)[165:]\n",
    "    tokenized_data = list(map(lambda x: x.lower(), tokenized_data))\n",
    "    tokenized_data = list(map(lambda x: re.sub('[^A-Za-z0-9]+', '', x), tokenized_data))\n",
    "    tokenized_data = [w for w in tokenized_data if len(w)]\n",
    "    return tokenized_data\n",
    "\n",
    "def sub_word_tokenize(data):\n",
    "    tokenizer = BertWordPieceTokenizer()\n",
    "    tokenizer.train_from_iterator(data, vocab_size=10000)\n",
    "    tokenized_data = tokenizer.encode(\" \".join(data))\n",
    "    return tokenizer, tokenized_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ShakespeareDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_ids, sequence_length, device):\n",
    "\n",
    "        self.words_indexes = data_ids\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.sequence_length], device=self.device),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1], device=self.device)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, n_vocab, device):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm_size = 512\n",
    "        self.embedding_dim = 100\n",
    "        self.num_layers = 2\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = preprocess('/home/maria/Documents/NLP/data/assignment_5/pg100.txt')\n",
    "tokenizer, tokenized_data = sub_word_tokenize(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ShakespeareDataset(tokenized_data.ids, SEQUENCE_LENGTH, device)\n",
    "model = LSTMModel(10000, device)\n",
    "model.to(device)\n",
    "# trained for 20 epochs ~4h?\n",
    "model.load_state_dict(torch.load('/home/maria/Documents/NLP/data/assignment_5/shakespeare_2.model'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "prefixes_sufixes = {}\n",
    "for word in data:\n",
    "    e = tokenizer.encode(word)\n",
    "    if len(e.ids) > 1:\n",
    "        prefixes_sufixes[e.ids[0]] = e.ids[1:]\n",
    "sufixes_ids = []\n",
    "for i in range(10000):\n",
    "    if tokenizer.id_to_token(i)[:2] == \"##\":\n",
    "        sufixes_ids.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def top_p_sampling(p, top_p):\n",
    "    sorted_logits, sorted_indices = torch.sort(torch.from_numpy(p), descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    p[sorted_indices[sorted_indices_to_remove]] = 0.0\n",
    "    p = p/p.sum()\n",
    "    return p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, text, prefixes_sufixes, next_words=15):\n",
    "    model.eval()\n",
    "\n",
    "    words = tokenizer.encode(text).ids\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    continuation = []\n",
    "    existing_3_grams = set(ngrams(words, 3))\n",
    "    for i in range(next_words):\n",
    "\n",
    "        x = torch.tensor([words[i:]])\n",
    "        x = x.to(device)\n",
    "\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "        next_from_continuation = -1\n",
    "        if tokenizer.decode([words[-1]]) not in data and len(continuation) > 0:\n",
    "            words.append(continuation.pop())\n",
    "        else:\n",
    "            last_word_logits = y_pred[0][-1]\n",
    "            p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "\n",
    "            to_exclude = set(sufixes_ids)\n",
    "            if len(continuation) > 0:\n",
    "                next_from_continuation = continuation.pop(0)\n",
    "                to_exclude = to_exclude - {next_from_continuation}\n",
    "\n",
    "            to_exclude = list(to_exclude)\n",
    "            current_2gram = words[2:]\n",
    "            for ngram in existing_3_grams:\n",
    "                if ngram[0] == current_2gram[0] and ngram[1] == current_2gram[1]:\n",
    "                    to_exclude.append(ngram[2])\n",
    "\n",
    "            p[np.isin(np.arange(len(p)), to_exclude)] = 0\n",
    "            p = p/p.sum()\n",
    "            p = top_p_sampling(p, 0.9)\n",
    "            word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "            words.append(word_index)\n",
    "            if not word_index == next_from_continuation:\n",
    "                continuation = []\n",
    "            if word_index in prefixes_sufixes:\n",
    "                continuation = prefixes_sufixes[word_index]\n",
    "        existing_3_grams.add(tuple(words[3:]))\n",
    "    if tokenizer.decode([words[-1]]) not in data and len(continuation) > 0:\n",
    "        words.extend(continuation)\n",
    "    return words, tokenizer.decode(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "prompt = \"to be or not to be\"\n",
    "words, text = predict(model, tokenizer, prompt, prefixes_sufixes, 30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be that seed this room nor kneel d duty o err tars water on such car and see poor negligent brethren then white 6 d in the way thick d\n",
      "to True\n",
      "be True\n",
      "or True\n",
      "not True\n",
      "to True\n",
      "be True\n",
      "that True\n",
      "seed True\n",
      "this True\n",
      "room True\n",
      "nor True\n",
      "kneel True\n",
      "d True\n",
      "duty True\n",
      "o True\n",
      "err True\n",
      "tars False\n",
      "water True\n",
      "on True\n",
      "such True\n",
      "car True\n",
      "and True\n",
      "see True\n",
      "poor True\n",
      "negligent True\n",
      "brethren True\n",
      "then True\n",
      "white True\n",
      "6 True\n",
      "d True\n",
      "in True\n",
      "the True\n",
      "way True\n",
      "thick True\n",
      "d True\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "for w in text.split():\n",
    "    print(w, w in data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our doubts are traitors and make a reason follow when i should look my vow i have sat up my prison to the alasterab shore let my proud clear rage smile once more that\n"
     ]
    }
   ],
   "source": [
    "prompt = \"our doubts are traitors and make\"\n",
    "words, text = predict(model, tokenizer, prompt, prefixes_sufixes, 30)\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it never hurts to keep looking for sunshine of soldier where you are with welcome instateig d here mansion enter the old countess and attendant distur on the ground in his life princess of arc servant\n"
     ]
    }
   ],
   "source": [
    "prompt = \"it never hurts to keep looking for sunshine\"\n",
    "words, text = predict(model, tokenizer, prompt, prefixes_sufixes, 30)\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}