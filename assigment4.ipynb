{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef34449",
   "metadata": {},
   "source": [
    "### Assigment 4\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* get at least 4 points by Tuesday, 12.05.2022\n",
    "* remaining points: last lab session before or on Tuesday, 19.05.2022\n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files are on Drive: <https://drive.google.com/drive/folders/1HaMbhzaBxxNa_z_QJXSDCbv5VddmhVVZ?usp=sharing> (or will be soon :) )\n",
    "\n",
    "## Task 1 (5 points)\n",
    "\n",
    "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left. The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
    "\n",
    "<pre>\n",
    "word1 x1_1 x1_2 ... x1_N \n",
    "word2 x2_1 x2_2 ... x2_N\n",
    "...\n",
    "wordK xK_1 xK_2 ... xk_N\n",
    "</pre>\n",
    "\n",
    "Use the loss from Slide 3 in Lecture NLP.2, compute the gradient manually. You can use some gradient clipping, or regularisation. \n",
    "\n",
    "**Remark**: the data is specially prepared to make the learning process easier. \n",
    "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "\n",
    "    def __init__(self, data, dim, lr):\n",
    "\n",
    "        self._data = data\n",
    "        self._object_encoder = LabelEncoder()\n",
    "        self._data['object'] = self._object_encoder.fit_transform(data['object'])\n",
    "        self._context_encoder = LabelEncoder()\n",
    "        self._data['context'] = self._context_encoder.fit_transform(data['context'])\n",
    "        self._objects = np.unique(data[['object']].values)\n",
    "        self._contexts = np.unique(data[['context']].values)\n",
    "        self._vocab_count = None\n",
    "        self._vocab_prob = None\n",
    "        self._positives = None\n",
    "        self._calculate_sampling_prob()\n",
    "        self._prepare_positives()\n",
    "\n",
    "        self._lr = lr\n",
    "        self._dim = dim\n",
    "        self._Wo = np.random.rand(len(self._objects), dim)\n",
    "        self._Wc = np.random.rand(len(self._contexts), dim)\n",
    "\n",
    "    def _calculate_sampling_prob(self):\n",
    "\n",
    "        self._vocab_count = defaultdict(int)\n",
    "\n",
    "        for word in self._data['object']:\n",
    "            self._vocab_count[word] += 1\n",
    "\n",
    "        norm = sum([freq**(3/4) for freq in self._vocab_count.values()])\n",
    "        self._vocab_prob = {word: (freq** (3 / 4) / norm) for word, freq in self._vocab_count.items()}\n",
    "\n",
    "    def _prepare_positives(self):\n",
    "\n",
    "        self._positives = defaultdict(set)\n",
    "        for _, row in tqdm(self._data.iterrows()):\n",
    "            self._positives[row.context] |= {row.object}\n",
    "\n",
    "    def update_object_embedding(self, word_idx, gradient):\n",
    "\n",
    "        self._Wo[word_idx, :] -= self._lr * gradient\n",
    "\n",
    "    def update_context_embedding(self, word_idx, gradient):\n",
    "\n",
    "        self._Wc[word_idx, :] -= self._lr * gradient\n",
    "\n",
    "    def get_negatives_sample(self, k):\n",
    "\n",
    "        negatives = np.random.choice(list(self._vocab_prob.keys()), size=(len(self._data), k), p=list(self._vocab_prob.values()))\n",
    "\n",
    "        return negatives\n",
    "\n",
    "    def step(self, idx, negatives):\n",
    "        word_idx, context_idx = self._data.iloc[idx]\n",
    "        negatives = list(set(negatives) - set(self._positives[context_idx]))\n",
    "\n",
    "        loss = -np.log(sigmoid(self._Wo[word_idx, :] @ self._Wc[context_idx, :])) - np.sum(np.log(sigmoid(-self._Wo[negatives, :] @ self._Wc[context_idx, :])))\n",
    "        derivative_object = -self._Wc[context_idx, :]*(1 - sigmoid(self._Wo[word_idx, :] @ self._Wc[context_idx, :]))\n",
    "        derivative_context = -self._Wo[word_idx, :]*(1 - sigmoid(self._Wo[word_idx, :] @ self._Wc[context_idx, :])) \\\n",
    "                             + np.sum(self._Wo[negatives, :].T * (1 - sigmoid(-self._Wc[context_idx, :].reshape(1, -1) @ self._Wo[negatives, :].T)), axis=1)\n",
    "\n",
    "        self.update_object_embedding(word_idx, derivative_object)\n",
    "        self.update_context_embedding(context_idx, derivative_context)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def save_embeddings(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(f'{str(len(self._objects))} {str(self._dim)} \\n')\n",
    "            for word in self._objects:\n",
    "                embedding = self._Wo[word]\n",
    "                f.write(f'{self._object_encoder.inverse_transform([word])[0]} {\" \".join(list(map(str, list(embedding))))}  \\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data = pd.read_csv('task1_objects_contexts_polish.txt', sep=' ', names=['object', 'context'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "468df83f9a4b47f19db79ab7ba411c5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = Word2Vec(data, 20, 0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe9eb6e6f3774cfb974f14ecf3d6ac58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.07505048998839\n",
      "EPOCH 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "079cf7fdb3ba4e05aa6322d28303d660"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.205103797833125\n",
      "EPOCH 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6cbbf53b2544e17a3bfb31c0c6302b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.208122511127881\n",
      "EPOCH 3\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd157439b6524cba8a47c0981e3965ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.15037952256908\n",
      "EPOCH 4\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "844d4b82721a40aab4f788124b79ea09"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.102408981545655\n",
      "EPOCH 5\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e37e7c043f8d42cb9fec6568a6ddfa30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0664201420036274\n",
      "EPOCH 6\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8c742f9bab349f08b6330b504b5058a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.037111534128647\n",
      "EPOCH 7\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c457760cbe4042488197aaa08d17f905"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.015244475996828\n",
      "EPOCH 8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b73738e657442b7b95f9f15dc2bb952"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.99764462159789\n",
      "EPOCH 9\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d380d26ad8504ae2aaffe650d82ee69b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9853046416755697\n",
      "EPOCH 10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cb62e2561f64f55b2e83591b59ed469"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.976390358595205\n",
      "EPOCH 11\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e69c09c2a4fc48dca49fbc39358ceb31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9686737359713704\n",
      "EPOCH 12\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77e9e9db0163417b9ab15d186d8d97fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.964321558658121\n",
      "EPOCH 13\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c91f849bb6f94c5394d05dcc3fe82a59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.962011747205926\n",
      "EPOCH 14\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "100e97ddd25e458482ba64c07c9496a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.960365536200353\n",
      "EPOCH 15\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8be80327a1342eeaf8a7a55627596ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9600800851822706\n",
      "EPOCH 16\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6bca6765067a4caf918a7962a3bf0d3c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9605015391904077\n",
      "EPOCH 17\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e19cca331df740c6982523cceb1f240b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.961706487544347\n",
      "EPOCH 18\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8add8b5c7b2d490e9517e4af03693225"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.963314780543266\n",
      "EPOCH 19\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44fc4b0c8dfc41ffb61ad3b0de9c4636"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9665752325498493\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    negatives = w.get_negatives_sample(5)\n",
    "    print(f\"EPOCH {epoch}\")\n",
    "    losses = []\n",
    "    for i in tqdm(range(len(negatives))):\n",
    "        loss = w.step(i, negatives[i])\n",
    "        losses.append(loss)\n",
    "    print(np.mean(np.array(losses)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "w.save_embeddings('task1_w2v_vectors.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: pies\n",
      "    koń 0.9517124891281128\n",
      "    kot 0.9100876450538635\n",
      "    krowa 0.9069613814353943\n",
      "    zwierzę 0.8956605195999146\n",
      "    baba 0.8795665502548218\n",
      "    dziewczyna 0.8694310784339905\n",
      "    mężczyzna 0.8668525815010071\n",
      "    ptak 0.8655155301094055\n",
      "    chłopiec 0.8645903468132019\n",
      "    facet 0.8518747687339783\n",
      "\n",
      "WORD: smok\n",
      "    potwór 0.8994752764701843\n",
      "    niedźwiedź 0.896720826625824\n",
      "    dziewczę 0.896420419216156\n",
      "    kot 0.8909311890602112\n",
      "    maluch 0.8871726989746094\n",
      "    panienka 0.8804367780685425\n",
      "    dzieciak 0.877396285533905\n",
      "    mucha 0.8752036690711975\n",
      "    wiedźma 0.8732087016105652\n",
      "    tygrys 0.869754433631897\n",
      "\n",
      "WORD: miłość\n",
      "    wiara 0.8628854155540466\n",
      "    duch 0.7667196393013\n",
      "    prawda 0.766288161277771\n",
      "    bóg 0.760909914970398\n",
      "    wyobraźnia 0.7606624364852905\n",
      "    młodość 0.7587139010429382\n",
      "    radość 0.7577516436576843\n",
      "    zło 0.7552950382232666\n",
      "    uczucie 0.7470588088035583\n",
      "    przyjaźń 0.7453605532646179\n",
      "\n",
      "WORD: rower\n",
      "    motocykl 0.9155734777450562\n",
      "    wózek 0.9037888646125793\n",
      "    telewizor 0.9026089310646057\n",
      "    limuzyna 0.8952810168266296\n",
      "    wóz 0.8825356364250183\n",
      "    auto 0.8778617978096008\n",
      "    ciężarówka 0.8661344647407532\n",
      "    wanna 0.8539314866065979\n",
      "    trumna 0.8537191152572632\n",
      "    namiot 0.8506611585617065\n",
      "\n",
      "WORD: maraton\n",
      "    rajd 0.870089590549469\n",
      "    piknik 0.8687446713447571\n",
      "    jubileusz 0.8647351264953613\n",
      "    obchód 0.863508939743042\n",
      "    potyczka 0.8594064712524414\n",
      "    turniej 0.8574094176292419\n",
      "    set 0.8568875789642334\n",
      "    festyn 0.8528019785881042\n",
      "    jarmark 0.8525531888008118\n",
      "    sparing 0.8522839546203613\n",
      "\n",
      "WORD: logika\n",
      "    złożoność 0.8525761365890503\n",
      "    filozofia 0.8508092761039734\n",
      "    etyka 0.8497163653373718\n",
      "    geneza 0.8474440574645996\n",
      "    ideał 0.8469904065132141\n",
      "    specyfika 0.8436583876609802\n",
      "    orientacja 0.8379095792770386\n",
      "    odmienność 0.8348488807678223\n",
      "    zagadnienie 0.8275545835494995\n",
      "    pojmowanie 0.8215750455856323\n",
      "\n",
      "WORD: motyl\n",
      "    ptak 0.9180769920349121\n",
      "    owad 0.9126334190368652\n",
      "    wąż 0.90543133020401\n",
      "    żółw 0.9046567678451538\n",
      "    mrówka 0.8719978332519531\n",
      "    ślimak 0.865896463394165\n",
      "    żaba 0.8539764285087585\n",
      "    potwór 0.849324643611908\n",
      "    mysz 0.8475809693336487\n",
      "    tygrys 0.8447991609573364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors.txt', binary=False)\n",
    "\n",
    "example_english_words = ['dog', 'dragon', 'love', 'bicycle', 'marathon', 'logic', 'butterfly']  # replace, or add your own examples\n",
    "example_polish_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
    "\n",
    "example_words = example_polish_words\n",
    "\n",
    "for w0 in example_words:\n",
    "    print ('WORD:', w0)\n",
    "    for w, v in task1_wv.most_similar(w0):\n",
    "        print ('   ', w, v)\n",
    "    print ()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "41228961",
   "metadata": {},
   "source": [
    "## Task 2 (4 points)\n",
    "\n",
    "Your task is to train the embeddings for Simple Wikipedia titles, using gensim library. As the example below shows, training is really simple:\n",
    "\n",
    "```python\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "```\n",
    "*sentences* can be a list of list of tokens, you can also use *gensim.models.word2vec.LineSentence(source)* to create restartable iterator from file. At first, use [this file] containing such pairs of titles, that one article links to another.\n",
    "\n",
    "We say that two titles are *related* if they both contain a word (or a word bigram) which is not very popular (it occurs only in several titles). Make this definition more precise, and create the corpora which contains pairs of related titles. Make a mixture of the original corpora, and the new one, then train title vectors again.\n",
    "\n",
    "Compare these two approaches using similar code to the code from Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f6ad8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cell for your presentation\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "wiki = pd.read_csv('simple_wiki_links.txt', sep=' ', names=['object', 'context'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "titles = list(set(map(str, list(wiki.object) + list(wiki.context))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "titles_series = pd.Series(titles)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "word_counts = titles_series.apply(lambda row: str(row).split('_')).explode().value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "rare_words = set(word_counts[(word_counts <= 20) & (word_counts > 1)].index)\n",
    "rare_words = set([w for w in rare_words if len(w) > 3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "193022"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rare_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "index = defaultdict(list)\n",
    "for idx, title in enumerate(titles):\n",
    "    for rare_word in list(set(title.split('_')) & rare_words):\n",
    "        index[rare_word].append(idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/193022 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "300757a24a0d4cf1a7ff7118ffb6eb50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pairs = set()\n",
    "for titles_list in tqdm(index.values()):\n",
    "    current_pairs = itertools.combinations(titles_list, 2)\n",
    "    for pair in current_pairs:\n",
    "        pairs.add(pair)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "with open('task2_rare_words_corpus.txt', 'w') as f:\n",
    "    for title_1, title_2 in pairs:\n",
    "        f.write(f'{titles[title_1]} {titles[title_2]} \\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "with open('task2_merged_corpus.txt','wb') as wfd:\n",
    "    for f in ['task2_rare_words_corpus.txt','simple_wiki_links.txt']:\n",
    "        with open(f,'rb') as fd:\n",
    "            shutil.copyfileobj(fd, wfd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "raw_data = LineSentence('simple_wiki_links.txt')\n",
    "model_1 = Word2Vec(sentences=raw_data, vector_size=50, epochs=20, window=2, min_count=1, workers=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "corpus_data = LineSentence('task2_merged_corpus.txt')\n",
    "model_2 = Word2Vec(sentences=corpus_data, vector_size=50, epochs=20, window=2, min_count=1, workers=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "rare_data = LineSentence('task2_rare_words_corpus.txt')\n",
    "model_3 = Word2Vec(sentences=corpus_data, vector_size=50, epochs=20, window=2, min_count=1, workers=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model_1.save('task2_model1')\n",
    "model_2.save('task2_model2')\n",
    "model_3.save('task2_model3')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: hyperinflation\n",
      "MODEL 0: \n",
      "world_war_i_reparations\n",
      "mv_rachel_corrie\n",
      "kristallnacht\n",
      "diktat\n",
      "kurt_schumacher\n",
      "MODEL 1: \n",
      "allied_powers_of_world_war_i\n",
      "herbert_backe\n",
      "putsch\n",
      "planned_economy\n",
      "attac_(organization)\n",
      "MODEL 2: \n",
      "reichsbürger_movement\n",
      "world_war_i_reparations\n",
      "may_coup\n",
      "nsdap_25_points_manifesto\n",
      "asylum_seeker\n",
      "\n",
      "WORD: kevin_spacey\n",
      "MODEL 0: \n",
      "holly_hunter\n",
      "oliver_stone\n",
      "matthew_broderick\n",
      "jodie_foster\n",
      "ray_liotta\n",
      "MODEL 1: \n",
      "holly_hunter\n",
      "jack_nicholson\n",
      "oliver_stone\n",
      "jodie_foster\n",
      "ed_harris\n",
      "MODEL 2: \n",
      "oliver_stone\n",
      "holly_hunter\n",
      "chicago_film_critics_association_awards_1990\n",
      "jodie_foster\n",
      "ray_liotta\n",
      "\n",
      "WORD: animal\n",
      "MODEL 0: \n",
      "insect\n",
      "species\n",
      "mammal\n",
      "chordate\n",
      "arthropod\n",
      "MODEL 1: \n",
      "species\n",
      "insect\n",
      "mammal\n",
      "chordate\n",
      "arthropod\n",
      "MODEL 2: \n",
      "species\n",
      "insect\n",
      "mammal\n",
      "arthropod\n",
      "teeth\n",
      "\n",
      "WORD: catholicism\n",
      "MODEL 0: \n",
      "protestantism\n",
      "protestant\n",
      "roman_catholicism\n",
      "evangelicalism\n",
      "christian_denomination\n",
      "MODEL 1: \n",
      "lutheranism\n",
      "protestantism\n",
      "protestant\n",
      "christian_denomination\n",
      "evangelicalism\n",
      "MODEL 2: \n",
      "protestantism\n",
      "protestant\n",
      "lutheranism\n",
      "evangelicalism\n",
      "christian_denomination\n",
      "\n",
      "WORD: nonprofit_organization\n",
      "MODEL 0: \n",
      "loan\n",
      "advocacy\n",
      "law_firm\n",
      "social_security\n",
      "501(c)(3)\n",
      "MODEL 1: \n",
      "macroeconomics\n",
      "501(c)(3)\n",
      "foundation_(nonprofit)\n",
      "think_tank\n",
      "bail\n",
      "MODEL 2: \n",
      "501(c)(3)\n",
      "nonprofit\n",
      "macroeconomics\n",
      "non-profit\n",
      "non-profit_organisation\n",
      "\n",
      "WORD: critters_(film)\n",
      "MODEL 0: \n",
      "ranger_john_smith\n",
      "characters_of_kingdom_hearts#wonderland\n",
      "alpha_trion\n",
      "judge_claude_frollo\n",
      "shockwave_(transformers)\n",
      "MODEL 1: \n",
      "hatter_(alice's_adventures_in_wonderland)\n",
      "tenchi_the_movie:_tenchi_muyo_in_love\n",
      "category:teenage_mutant_ninja_turtles_characters\n",
      "tenchi_muyo!_daughter_of_darkness\n",
      "the_amazing_spider-man_vs._the_kingpin\n",
      "MODEL 2: \n",
      "hatter_(alice's_adventures_in_wonderland)\n",
      "category:teenage_mutant_ninja_turtles_characters\n",
      "walt_disney_world_quest:_magical_racing_tour\n",
      "meet_the_robinsons_(video_game)\n",
      "time_trapper\n",
      "\n",
      "WORD: radosław_wojtaszek\n",
      "MODEL 0: \n",
      "anna_muzychuk\n",
      "pavel_kohout\n",
      "antoaneta_stefanova\n",
      "pavel_eljanov\n",
      "gustaw_lutkiewicz\n",
      "MODEL 1: \n",
      "nodiko_tatishvili\n",
      "dmitry_jakovenko\n",
      "anna_muzychuk\n",
      "ian_nepomniachtchi\n",
      "nikita_vitiugov\n",
      "MODEL 2: \n",
      "pavel_eljanov\n",
      "antoaneta_stefanova\n",
      "anna_muzychuk\n",
      "nikita_vitiugov\n",
      "pavel_kohout\n",
      "\n",
      "WORD: spotted_sandpiper\n",
      "MODEL 0: \n",
      "hoary_bat\n",
      "common_ground_dove\n",
      "long-tailed_weasel\n",
      "north_american_river_otter\n",
      "bushtit\n",
      "MODEL 1: \n",
      "long-tailed_weasel\n",
      "amorpha_juglandis\n",
      "common_ground_dove\n",
      "trifolium_pratense\n",
      "paramylodon\n",
      "MODEL 2: \n",
      "long-tailed_weasel\n",
      "amorpha_juglandis\n",
      "north_american_river_otter\n",
      "trifolium_pratense\n",
      "larinus_planus\n",
      "\n",
      "WORD: celts_druid\n",
      "MODEL 0: \n",
      "phrygian_language\n",
      "enserune\n",
      "diogenes_laertius\n",
      "klagspiegel\n",
      "claudia_julia\n",
      "MODEL 1: \n",
      "druid_(band)\n",
      "druid_(database_designer)\n",
      "mount_druid\n",
      "druid_(world_of_warcraft)\n",
      "druid_(computer_game)\n",
      "MODEL 2: \n",
      "mount_druid\n",
      "druid_(computer_game)\n",
      "druid_(band)\n",
      "druid_theatre_company\n",
      "north_druid_hills,_georgia\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_titles = [\"hyperinflation\",\n",
    "                  \"kevin_spacey\",\n",
    "                  \"animal\",\n",
    "                  \"catholicism\",\n",
    "                  \"nonprofit_organization\",\n",
    "                  \"critters_(film)\",\n",
    "                  \"radosław_wojtaszek\",\n",
    "                  'spotted_sandpiper',\n",
    "                  'celts_druid']\n",
    "models = [model_1, model_2, model_3]\n",
    "for word in example_titles:\n",
    "    print(f'WORD: {word}')\n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"MODEL {i}: \")\n",
    "        try:\n",
    "            similar = model.wv.most_similar(word, topn=5)\n",
    "        except Exception:\n",
    "            similar = []\n",
    "        for similar_word, score in similar:\n",
    "            print(similar_word)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "{'brickley',\n 'olekminsky',\n 'venediktov',\n 'mcilrath',\n 'marlène',\n 'huna',\n 'beekmantown,',\n 'fitzhugh',\n 'khana',\n 'conrado',\n 'verfeil,',\n 'torbjörn',\n 'križevci,',\n 'file:h&amp;c',\n 'cockroach',\n 'burce',\n 'crittall',\n 'osmers',\n 'hurford',\n 'ozon,',\n \"slash's\",\n 'category:k-pop',\n '1501',\n 'verreaux',\n 'vendt',\n 'template:airport',\n 'hickel',\n 'takahito',\n 'claudel',\n 'taulé',\n 'ojibwa',\n 'slobozia',\n 'garlands',\n 'category:1437',\n 'yari',\n 'diatomic',\n 'file:1950',\n 'percent/doc',\n 'stringfield',\n 'image:eshtaol',\n 'zhabotinsky',\n \"l'aigle\",\n 'staring',\n '(horseracing)',\n 'morang',\n 'hallelujah,',\n 'passat',\n 'daladier',\n 'troupe',\n '639-3',\n 'file:tudor',\n 'lieberknecht',\n 'calais,',\n 'sideline',\n 'lacquerware',\n 'romblon',\n 'redoubt',\n '1933)',\n 'consilience',\n 'multiethnic',\n 'juillé,',\n 'buckenham',\n 'downloads',\n 'aketi',\n 'ponto',\n 'taref',\n 'cuellar',\n 'ustaad',\n 'zizzo',\n 'wp:irc',\n 'template:cooper',\n 'zorica',\n 'lenox,',\n 'template:second',\n 'yenice,',\n 'krishianis',\n 'venetia',\n 'harlington',\n 'worldcon',\n 'vecta',\n 'file:hiiumaa',\n 'kutzenhausen,',\n 'caviar',\n 'goncharov',\n 'collipark',\n 'ro-1',\n 'teat',\n '1018',\n 'emission.png',\n 'carnation',\n 'excerpt/sandbox',\n 'valls',\n 'topography',\n 'brandt,',\n 'rieti',\n 'wenden',\n 'rochebrune',\n 'skybus',\n 'clemerson',\n 'traci',\n 'leiva',\n 'crochu',\n ':{{ucfirst:opening',\n 'regard',\n 'plotter',\n 'file:traffic',\n 'misinformation',\n 'cherryvale,',\n 'view)',\n 'eetion',\n 'testa',\n 'hložek',\n 'template:butler',\n 'mccafferty',\n 'tamano',\n 'kasseb',\n 'file:visit',\n '(electrical)',\n 'einecs',\n 'earnie',\n 'prust',\n 'brandywine,',\n 'quinte',\n 'rijnders',\n 'desperation',\n 'edmondo',\n 'helveticus',\n '(printmaking)',\n '17}}}',\n 'sebastián,',\n 'canonic',\n 'srirangam',\n 'hakan',\n 'category:economics',\n 'category:comic',\n 'lytovchenko',\n 'cunningham:',\n 'poorhouse',\n ':en:template:taxonomy',\n 'northcott',\n 'onondaga',\n 'jacki',\n 'file:complete',\n 'file:montealegre',\n 'fyodorovna',\n 'template:respect',\n 'midair',\n 'iranica',\n ':category:musicians',\n 'seule',\n 'najera',\n 'imperio',\n 'korngold',\n 'pêche',\n 'yunnan,',\n 'alessandria',\n 'mile,',\n 'burera',\n 'strassman',\n 'planchart',\n 'berryton,',\n 'morazán,',\n 'pitre',\n '3166/data/cy',\n '(priest)',\n 'čeferin',\n 'ch-nw',\n 'nipponica',\n 'óbuda',\n 'image:putten',\n '(cook',\n 'vignette',\n 'noumiso',\n 'hoeness',\n 'anoop',\n '5.56x45mm',\n 'palaestina',\n 'swagger',\n 'ravensdale,',\n 'poster/doc',\n 'template:bus',\n 'extract/gl',\n 'mcglynn',\n 'munakata',\n '1921)',\n \"turnin'\",\n '1601',\n 'viraj',\n 'marquand',\n 'tennessee,',\n 'vinet',\n 'tubules',\n 'parietal',\n 'montebourg',\n 'judson,',\n 'wikified)',\n 'principlists',\n 'file:jean-marc',\n 'bible.gif',\n 'kunihiko',\n 'omdurman',\n 'guinn',\n 'file:octagonal',\n 'template:hockey',\n 'barauli',\n 'template:ahf',\n 'garrow',\n 'kotor',\n 'prionailurus',\n 'kerri',\n 'philippopolis',\n 'america}}',\n 'fidelio',\n 'o.m.',\n 'mw:manual',\n 'ball#dodoria',\n 'plater',\n 'teachout',\n 'file:stuart',\n '1148',\n 'gento',\n 'refuge,',\n 'wachsberger',\n '(avec',\n 'mansingh',\n 'file:nürnberg',\n 'vilija',\n 'faik',\n '1799',\n 'ramadi',\n 'mittleres',\n 'agenesis',\n 'category:unusual',\n 'sapien',\n 'timex',\n 'chairpersons',\n 'series)#future',\n 'reuss-ebersdorf',\n 'empresses',\n 'cabannes,',\n 'trechus',\n 'octopussy',\n 'neuendorf',\n 'honduran',\n 'onesimus',\n '90th',\n 'bencoolen',\n 'category:1349',\n 'motogp',\n 'choge',\n 'alvord,',\n 'böhm',\n 'prespa',\n 'amaral,',\n 'petit-verly',\n '(century)',\n 'vuillaume',\n 'vocabulary',\n 'meiju',\n 'baltea',\n 'cristopher',\n 'batrachoseps',\n 'ballou,',\n 'messager',\n '(paranã',\n 'chartreuse',\n 'järvsö',\n 'levett',\n 'torinese.jpg',\n 'keszler',\n \"ra'anana\",\n 'samverkan',\n 'lahab',\n 'mastodon',\n '2997',\n 'sollefteå',\n 'scudo',\n 'baya',\n 'titles#tun',\n 'izzat',\n 'bécquer',\n 'jihlava',\n 'maiteeq',\n 'kudian,',\n 'organism',\n 'impalement',\n 'nova,',\n 'drosera',\n 'mchenry,',\n 'dehlvi',\n 'koopa',\n 'urawa-ku,',\n 'heythrop',\n 'deletion/requests/2020/mike',\n 'brandreth',\n 'wendouree',\n 'ekimae',\n '(pillar',\n 'lory',\n 'priebus',\n \"nyong'o\",\n 'koff',\n 'diomedea',\n 'halperin',\n 'champ,',\n 'zenica',\n 'methven',\n 'beernaert',\n 'cavarozzi',\n 'singaporeans',\n 'skeleton,',\n 'buff.ogg',\n 'abhijit',\n '(newscaster)',\n 'wail',\n '(translator)',\n 'file:janez',\n '1724',\n 'lab.jpg',\n 'hitlerjugend',\n 'shoval',\n 'masaccio',\n 'estate.jpg',\n 'jamui',\n 'muitalus',\n 'quercia',\n 'lindros',\n 'file:quentin',\n 'conchobair',\n 'cunningham.jpg',\n 'tabernacle,',\n 'mészáros',\n '(2016-present)',\n 'presenter#television',\n 'taichung',\n 'category:1642',\n 'kitzmiller,',\n 'bourg-de-péage',\n 'navigo',\n 'clontarf,',\n 'responsibly',\n 'bochco',\n 'selkup',\n 'birchwood,',\n 'zealand&amp;#39;s',\n 'suaçuí',\n 'shales',\n 'crowland',\n 'rthk',\n 'paganico',\n 'assistant)',\n 'pignanelli',\n 'file:phan',\n 'masatsugu',\n 'rutovu',\n 'pancarana',\n 'canby,',\n 'voironnais',\n 'seicento',\n 'jpmorgan',\n 'lindos',\n 'al-waleed',\n 'file:zambezi',\n 'rentería',\n 'matalon',\n 'signo',\n 'liebergot',\n 'streicher',\n 'westhausen,',\n 'artesian',\n 'burghley',\n 'lichtenau',\n 'chawhan',\n 'loizaga',\n 'catwoman',\n 'leptinotarsa',\n 'techo',\n 'atacames.png',\n 'babych',\n 'meleager',\n '509001–510000',\n 'foch',\n 'yunta',\n 'category:121',\n 'fourmi',\n 'tozama',\n 'observer-reporter',\n 'schallplatten',\n '(turkey,',\n 'haripur,',\n 'gauchel',\n 'boole',\n 'citti',\n 'kancil',\n 'file:bleeding',\n 'egidien',\n 'skrapar',\n 'rominger',\n 'requesens',\n 'wynette',\n 'en-gb-5',\n 'file:millie',\n 'orientalists',\n 'kahaniyaan',\n 'chosun',\n 'kane,',\n 'gemkow',\n 'zarina',\n 'conchita',\n 'szczytno',\n 'file:pendulumdisplacementphases',\n 'greeley,',\n 'mortem',\n 'safwa',\n 'deepavali',\n 'shahin',\n 'cardinal,',\n 'anarcho',\n 'loznica',\n 'jumanji',\n 'shalyapin',\n 'eguileor',\n 'pázmány',\n '8500',\n 'daguerreotype',\n 'bibel',\n 'cauldwell',\n 'okuno',\n 'image:marina',\n 'gazzetta',\n 'fannrem',\n '&quot;..(args1',\n 'toptani',\n 'tallow',\n 'condyle',\n 'peafowl',\n 'egan,',\n 'rossa',\n 'seagram',\n 'feucht',\n 'hannu',\n 'bascuñán',\n 'levada',\n 'glimåkra',\n 'lomellina',\n 'fibre-reinforced',\n 'molluscs',\n 'simoncelli',\n 'dodon',\n 'file:dog',\n 'pharmaceuticals',\n '(sixteen',\n 'kübler',\n 'wham',\n 'houthi',\n 'halmstad',\n 'saint-quentin-en-yvelines',\n 'rheingau-taunus',\n 'egliswil.svg',\n 'nahshon',\n 'yellin',\n 'dawood',\n 'hagenbüchach',\n 'baaz',\n 'cuts',\n 'ghobadi',\n 'legislature#legislative',\n 'fontaine-française',\n 'sahrarud',\n 'sidibé',\n 'wits',\n 'respiration',\n 'damascus.jpg',\n 'cancuén',\n 'pikeville,',\n 'asotin',\n 'mammadaliyev',\n 'auxon',\n 'bookcase.svg',\n 'västanfors',\n 'evasion',\n 'alouette',\n 'template:editnotices/page/template',\n 'extasy',\n 'wiedemann',\n 'category:1370',\n 'kapitana',\n 'telescope,',\n 'frankincense',\n '#genre',\n 'trinidad.jpg',\n 'chaffee',\n 'toolworks',\n 'obelisk.jpg',\n 'vitebsk',\n 'arial',\n '44001–45000',\n 'tandoori',\n 'fothergill',\n \"cricketers'\",\n 'thye',\n 'armijo',\n 'putsch',\n 'pollachi',\n 'chanyu',\n 'seweryn',\n 'bertolini',\n 'dun-sur-auron',\n ':category:1776',\n 'fallax',\n '(nba',\n 'moik',\n 'puzhal',\n 'quantico',\n 'file:cervical',\n '#tanzania',\n 'vergne',\n 'switch#nintendo',\n 'milivoyev',\n 'file:riga',\n 'dubajić',\n 'roemer',\n 'booth,',\n 'file:handball',\n 'dalton,',\n 'namysłów',\n '(complexity)',\n 'buss',\n 'eleftheriou',\n 'marl',\n 'pa.jpg',\n 'image:hans',\n 'conyers',\n 'dahmani',\n 'bakke.jpg',\n 'file:niels',\n 'empalme',\n 'elfie',\n 'cahabón',\n 'rengo',\n 'agronomy',\n 'solbes',\n 'strålande',\n 'genji',\n 'njpw',\n 'grandin',\n 'nalbandian',\n 'barkan',\n 'category:bdsm',\n 'kerkhoffs',\n 'levene',\n ':en:jack',\n 'davis.jpg',\n 'janatha',\n 'nyarko',\n \"bear's\",\n 'potta',\n 'calendar)',\n 'frutos',\n 'horton.jpg',\n 'arcana',\n 'qutb-ud-din',\n 'laghetto',\n 'lahmar',\n 'congaree',\n 'anda,',\n 'anatole',\n 'sok.jpg',\n 'adlon',\n 'giengen',\n 'pokkudan',\n 'greenbrier,',\n '10.4',\n 'stormont,',\n 'zachariah',\n 'sarakham',\n 'province.png',\n 'aeneas',\n 'caála',\n '(2014–present)',\n 'file:beryl',\n 'caps',\n 'winona',\n 'cidra,',\n 'canapville,',\n 'tešanj',\n 'willetts',\n 'centrocaspian',\n 'decline',\n 'lochhead',\n 'balikpapan',\n 'datsik',\n 'organum',\n 'fernwald',\n '1923.png',\n 'devanahalli',\n 'solanum',\n 'fire,',\n 'regalado',\n 'file:l9',\n 't1.svg',\n '80286',\n 'raspe',\n 'hieroglyph',\n 'sejong',\n '2019-06-24.svg',\n 'stella,',\n 'sapucaiense',\n 'colusa',\n 'busoni,',\n 'category:1305',\n 'traunstein',\n 'restoration.png',\n 'ribadavia',\n 'gyeonggido',\n '(spy)',\n '(romance',\n 'ichnogenera',\n 'cosine',\n '&gt;--------------------',\n 'engmark',\n 'file:mikhail',\n 'tilloy-lez-marchiennes',\n 'missiroli',\n 'honcharenko',\n 'varga',\n 'schmiechen',\n 'lezant',\n '(1980s)',\n 'dee.svg',\n 'bondu',\n 'nərmin',\n 'löcknitz',\n 'kirkville,',\n 'praslinia',\n 'detectives',\n 'category:250s',\n 'bauza',\n 'conquista.svg',\n 'monastir',\n 'kaiserganj',\n 'noorpur,',\n 'paludan',\n 'sarov',\n 'horseheads',\n 'sarcostemma',\n 'heydari,',\n 'shum',\n 'reichsgau',\n 'melli',\n \"fintan's\",\n 'boveri',\n 'checkpoint',\n 'umaro',\n 'asllani',\n 'ester',\n '.303',\n 'northolt',\n 'alper',\n 'yotsuya',\n 'file:aiga',\n 'pelekai',\n 'timing',\n 'granlund',\n 'dentro',\n 'minver',\n 'volcanos',\n 'generali',\n 'froehlke',\n 'file:ad',\n 'mustafi',\n 'akashi',\n 'palazzina',\n 'image:provincia',\n 'taichi',\n 'lemar',\n 'prestes',\n 'serang.jpg',\n 'draft#supplemental',\n 'seventy-four',\n 'trissino',\n 'tibbets',\n '(waikato)',\n 'galperine',\n 'szekely',\n 'missing)',\n 'käerjeng',\n '1446',\n ':mni:ꯃꯤꯅ',\n 'savina',\n 'yachigusa',\n 'stoute',\n 'pesenti',\n 'delap',\n 'oppenheim',\n 'f.o.',\n 'file:orbit',\n 'gata',\n 'leskovar',\n 'thugs',\n 'wikipedia:do',\n '(sheep)',\n 'image:stoke',\n 'nursery',\n '(perm',\n 'mazin',\n 'paris-charenton',\n '(napoli).jpg',\n 'dalianidis',\n 'dazaifu',\n 'guyford',\n 'file:lafayette',\n 'kaizer',\n 'liliya',\n 'scaptia',\n 'rongorongo',\n 'setswana',\n 'ashu',\n 'olăneşti',\n 'chalamont',\n 'mal-e',\n 'tebbutt',\n 'heikki',\n 'bauernhof',\n 'thinktank',\n 'hardcover',\n '(1960)',\n 'begaye',\n 'milisavljevic',\n 'aspach,',\n 'talk:tim',\n 'lefh',\n 'chirodini',\n 'capell,',\n 'toons:',\n 'file:repin',\n 'dahlgren',\n 'pilar,',\n 'category:lang-x',\n 'syllable',\n 'spelare.jpg',\n 'file:pictogram',\n 'məmmədyarova',\n 'woodstar',\n 'punt',\n 'template:metropolitan',\n 'istočni',\n 'braderie',\n 'ogai',\n 'special:contributions/scott',\n 'drug/formatchembl',\n '(will.i.am',\n 'sahne',\n 'show#characters',\n 'pellworm',\n 'wrld',\n 'ganilau',\n '{{talkpagename:{{ucfirst:mohammad',\n 'andalusia,',\n 'machi',\n 'senescence',\n 'zari',\n 'l113',\n 'cathode',\n 'bañuelos',\n 'hamzeh',\n 'file:hermitage',\n 'roubion',\n 'category:1207',\n 'poitou-charentes',\n 'water-tube',\n 'oostflakkee.svg',\n 'bellew',\n 'claude-michel',\n '(delhi)',\n 'gaozong',\n 'lihua',\n 'phoca',\n 'technos',\n 'sorraia',\n 'francisci',\n 'hortensia',\n 'despenser,',\n 'baris',\n 'sinop,',\n 'sainte-hélène',\n 'grouchland',\n 'grandvaux',\n 'sphere.svg',\n 'trans-sur-erdre',\n 'pospisil',\n 'sinfonien06.ogg',\n 'feeding',\n 'file:cptm',\n 'elburzensis',\n 'grandvillars',\n 'tourtes',\n 'metternich',\n 'visualeditor',\n 'katha,',\n 'bipasha',\n 'maruhashi',\n 'file:uhličitan',\n 'aimard',\n 'rems',\n 'eiir.jpg',\n 'nits',\n 'act#schedule',\n 'kōyō',\n '-13th',\n 'file:inés',\n 'bushkan-e',\n 'lidingö',\n 'hirschthal',\n 'kanada',\n 'rescigno',\n 'metro-land',\n 'zygomatic',\n 'khruner.jpg',\n 'segni',\n 'ladywell',\n ':en:black',\n 'uncaf',\n 'eliomys',\n 'macrantha',\n 'brou',\n 'guttmann',\n 'nomans',\n 'anarkali',\n 'новосибирск',\n 'akeelah',\n 'longhorn',\n 'awards#golden',\n 'livermore,',\n 'manors,',\n 'knesset',\n 'deletion/requests/2022/amir',\n 'snowflakes',\n 'armidale',\n 'hvar',\n 'den-o:',\n 'shimer',\n 'kangari',\n 'reinsdorf,',\n 'file:mozambique',\n 'randstad',\n 'hasa',\n 'bottle,',\n 'prose',\n 'olympiads',\n 'category:areas',\n 'seki',\n 'hilo',\n 'alik',\n 'sendai,',\n 'himmelfahrt',\n 'lizio',\n 'interview.png',\n 'muharem',\n 'rocinha',\n 'bashundhara',\n 'aveiro',\n 'file:bougainville',\n 'mapper',\n 'nepomuceno',\n 'maliseet',\n 'sheung',\n 'dunwich',\n 'shankle',\n 'pischelsdorf',\n '(versailles)',\n 'category:string',\n 'riese',\n 'movement}}',\n 'bast',\n 'file:medical',\n '4x4:',\n 'menier',\n 'file:looe',\n 'file:inga',\n '2019-06-10.svg',\n 'bénac,',\n 'necklace',\n 'k.f.',\n 'sheinkin',\n 'gabrovo',\n 'khilji',\n 'volt',\n 'rankine',\n 'arlon',\n 'mabanda',\n 'spitzer',\n 'bourg',\n 'marsilio',\n 'courchamps,',\n 'cartmel',\n 'file:ayatollah',\n 'dinas',\n 'rosales,',\n 'file:juris',\n 'fafara',\n 'clovis,',\n 'taconic',\n 'mamayev',\n 'jm.jpg',\n 'topography.jpg',\n ':en:template:unbulleted',\n 'eshaq',\n 'template:cameroon',\n '(pitbull',\n 'lorton,',\n ':category:spider-man',\n 'secundum',\n 'aledo,',\n 'weaponry',\n 'couvin',\n '(2000s)',\n 'hamerton',\n 'radovljica',\n 'apam',\n 'chloë',\n 'month:',\n 'flourney,',\n 'rock)',\n 'image:aire',\n 'tarmstedt',\n 'yaroslav',\n 'policy#semi',\n 'chakravarty',\n 'balakrishna',\n 'zuñiga',\n 'stigma.jpg',\n 'scum',\n 'hell:',\n 'fresco.jpg',\n 'batu',\n 'fazbear',\n ':en:template:taxonomy/duomitus',\n 'berthen',\n 'tatry',\n 'element/symbol-to-top-image/testcases',\n 'emboscada',\n 'aboncourt,',\n 'soomro',\n 'cristaldo',\n 'horio',\n 'browne,',\n 'sedrata',\n 'n7.png',\n 'oolong',\n 'velba',\n 'states#florida',\n 'boardman',\n 'mofford',\n 'file:mongol',\n \"bowser's\",\n 'micro)/elementcell',\n 'file:swe',\n 'scoob',\n 'rear.jpg',\n 'byam',\n 'călăraşi',\n 'squamish-lillooet',\n 'blir',\n 'asuke',\n 'm271',\n 'impoundment',\n 'pressley',\n 'template:citequran',\n 'arlovski',\n 'wilbanks',\n 'kalispell,',\n 'sidwell',\n 'gallegos,',\n 'berlingo',\n 'peritonitis',\n 'al-ahram',\n 'sigg',\n 'valsugana',\n 'weingarten,',\n 'wishmaster',\n 'thiri',\n 'm:complete',\n 'ljubav',\n 'file:2007',\n 'gerhardsen',\n 'månsson',\n 'gwoyeu',\n 'kafi',\n 'itagaki',\n 'omnogovae',\n 'surja',\n 'fanfare',\n 'bainbridge',\n 'camporotondo',\n 'handball.jpg',\n 'park}}',\n 'comedian)',\n 'necessary',\n 'herveo',\n ...}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rare_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "d711fdca",
   "metadata": {},
   "source": [
    "# Task 3 (4 points)\n",
    "\n",
    "Suppose that we have two languages: Upper and Lower. This is an example Upper sentence:\n",
    "\n",
    "<pre>\n",
    "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\n",
    "</pre>\n",
    "\n",
    "And this is its translation into Lower:\n",
    "\n",
    "<pre>\n",
    "the quick brown fox jumps over the lazy dog\n",
    "</pre>\n",
    "\n",
    "You have two corpora for these languages (with different sentences). Your task is to train word embedings for both languages together, so as to make embeddings of the words which are its translations as close as possible. But unfortunately, you have the budget which allows you to prepare the translation only for 1000 words (we call it D, you have to deside which words you want to be in D)\n",
    "\n",
    "Prepare the corpora wich contains three kind of sentences:\n",
    "* Upper corpus sentences\n",
    "* Lower corpus sentences\n",
    "* sentences derived from Upper/Lower corpus, modified using D\n",
    "\n",
    "There are many possible ways of doing this, for instance this one (ROT13.COM: hfr rirel fragrapr sebz obgu pbecben gjvpr: jvgubhg nal zbqvsvpngvbaf, naq jvgu rirel jbeqf sebz Q ercynprq ol vgf genafyngvba)\n",
    "\n",
    "We define the score for an Upper WORD as  $\\frac{1}{p}$, where $p$ is a position of its translation in the list of **Lower** words most similar to WORD. For instance, when most similar words to DOG are:\n",
    "\n",
    "<pre>\n",
    "WOLF, CAT, WOLVES, LION, gopher, dog\n",
    "</pre>\n",
    "\n",
    "then the score for the word DOG is 0.5. Compute the average score separately for words from D, and for words out of D (hint: if the computation takes to much time do it for a random sample).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "with open('task3_polish_lower.txt', 'r') as f:\n",
    "    lower = f.readlines()\n",
    "with open('task3_polish_upper.txt', 'r') as f:\n",
    "    upper = f.readlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def preprocess(corpus):\n",
    "    processed_corpus = []\n",
    "    for sentence in corpus:\n",
    "        processed_sentence = \" \".join([w for w in sentence.strip().split(\" \") if w.isalpha()])\n",
    "        processed_corpus.append(processed_sentence)\n",
    "    return processed_corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "lower = preprocess(lower)\n",
    "upper = preprocess(upper)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "lower_tokens = [w.split(\" \") for w in lower]\n",
    "upper_tokens = [w.split(\" \") for w in upper]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model_lower = Word2Vec(sentences=lower_tokens, vector_size=50, epochs=20, window=5, min_count=1, workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "lower_vectors = model_lower.wv.vectors\n",
    "lower_clusters = KMeans(n_clusters=100, random_state=0).fit_predict(lower_vectors)\n",
    "lower_df = pd.DataFrame({\"words\": model_lower.wv.index_to_key, \"cluster\": lower_clusters})\n",
    "lower_counts = pd.Series(np.concatenate(lower_tokens)).value_counts()\n",
    "lower_df = lower_df.merge(lower_counts.rename(\"counts\"), left_on=\"words\", right_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "representatives = lower_df.groupby(['cluster']).apply(lambda group: group.sort_values('counts', ascending=False).iloc[:10]).words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "cluster      \n0        652            ziemia\n         796       miejscowość\n         835               las\n         871      powierzchnia\n         898           gatunek\n                     ...      \n99       787        zwolnienie\n         856         dochodowy\n         1017            zakup\n         1024            zwrot\n         1116    opodatkowanie\nName: words, Length: 994, dtype: object"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representatives"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "D_lower = set(representatives)\n",
    "D_upper = set(pd.Series(map(lambda x: x.upper(), representatives)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sentence in lower:\n",
    "    corpus.append(sentence)\n",
    "    sentence_set = set(sentence.split(\" \"))\n",
    "    in_D = sentence_set & D_lower\n",
    "    for to_translate in in_D:\n",
    "        translated = []\n",
    "        for w in sentence.split(\" \"):\n",
    "            if w == to_translate:\n",
    "                translated.append(w.upper())\n",
    "            else:\n",
    "                translated.append(w)\n",
    "        translated = \" \".join(translated)\n",
    "        if translated != sentence:\n",
    "            corpus.append(translated)\n",
    "\n",
    "for sentence in upper:\n",
    "    corpus.append(sentence)\n",
    "    sentence_set = set(sentence.split(\" \"))\n",
    "    in_D = sentence_set & D_upper\n",
    "    for to_translate in in_D:\n",
    "        translated = []\n",
    "        for w in sentence.split(\" \"):\n",
    "            if w == to_translate:\n",
    "                translated.append(w.lower())\n",
    "            else:\n",
    "                translated.append(w)\n",
    "        translated = \" \".join(translated)\n",
    "        if translated != sentence:\n",
    "            corpus.append(translated)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "with open('translated_corpus.txt', 'w') as f:\n",
    "    f.write('\\n'.join(corpus))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "corpus_data = LineSentence('translated_corpus.txt')\n",
    "model_corpus = Word2Vec(sentences=corpus_data , vector_size=50, epochs=20, window=5, min_count=1, workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "model_corpus.save('task3_model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def evaluate_lower(word, model):\n",
    "\n",
    "    similar = model.wv.most_similar(word, topn=1000)\n",
    "    i = 1\n",
    "    for w, s in similar:\n",
    "        if word == w.lower():\n",
    "            return 1 / i\n",
    "        if w.isupper():\n",
    "            i += 1\n",
    "    return 0\n",
    "\n",
    "def evaluate_upper(word, model):\n",
    "\n",
    "    similar = model.wv.most_similar(word, topn=1000)\n",
    "    i = 1\n",
    "    for w, s in similar:\n",
    "        if word == w.upper():\n",
    "            return 1 / i\n",
    "        if w.islower():\n",
    "            i += 1\n",
    "    return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9931466776851585\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for word in D_lower:\n",
    "    if word:\n",
    "        scores.append(evaluate_lower(word, model_corpus))\n",
    "print(np.array(scores).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9956719483568076\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for word in D_upper:\n",
    "    if word:\n",
    "        scores.append(evaluate_upper(word, model_corpus))\n",
    "print(np.array(scores).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4878803285718434\n"
     ]
    }
   ],
   "source": [
    "words_sample = lower_df.words\n",
    "for word in words_sample:\n",
    "    if word:\n",
    "        scores.append(evaluate_lower(word, model_corpus))\n",
    "print(np.array(scores).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46002020090028484\n"
     ]
    }
   ],
   "source": [
    "words_sample = list(set(np.concatenate(upper_tokens)))\n",
    "for word in words_sample:\n",
    "    if word:\n",
    "        scores.append(evaluate_upper(word, model_corpus))\n",
    "print(np.array(scores).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "4947e307",
   "metadata": {},
   "source": [
    "# Task 4 (4 points)\n",
    "\n",
    "In this task you are asked to do two things:\n",
    "1. compare the embeddings computed on small corpus (like Brown Corpus , see: <https://en.wikipedia.org/wiki/Brown_Corpus>) with the ones coming from Google News Corpus\n",
    "2. Try to use other resourses like WordNet to enrich to corpus, and obtain better embeddings\n",
    "\n",
    "You can use the following code snippets:\n",
    "\n",
    "```python\n",
    "# printing tokenized Brown Corpora\n",
    "from nltk.corpus import brown\n",
    "for s in brown.sents():\n",
    "    print(*s)\n",
    "    \n",
    "#iterating over all synsets in WordNet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for synset_type in 'avrns': # n == noun, v == verb, ...\n",
    "    for synset in list(wn.all_synsets(synset_type)))[:10]:\n",
    "        print (synset.definition())\n",
    "        print (synset.examples())\n",
    "        print ([lem.name() for lem in synset.lemmas()])\n",
    "        print (synset.hyperonims()) # nodes 1 level up in ontology\n",
    "        \n",
    "# loading model and compute cosine similarity between words\n",
    "\n",
    "model = Word2Vec.load('models/w2v.wordnet5.model') \n",
    "print (model.wv.similarity('dog', 'cat'))\n",
    "```\n",
    "\n",
    "Embeddings will be tested using WordSim-353 dataset, the code showing the quality is in the cell below. Prepare the following corpora:\n",
    "1. Tokenized Brown Corpora\n",
    "2. Definitions and examples from Princeton WordNet\n",
    "3. (1) and (2) together\n",
    "4. (3) enriched with pseudosentences containing (a subset) of WordNet knowledge (such as 'tiger is a carnivore')\n",
    "\n",
    "Train 4 Word2Vec models, and raport Spearman correletion between similarities based on your vectors, and similarities based on human judgements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947a2fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wordsim_relatedness_goldstandard.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m similarity_type \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelatedness\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m      9\u001B[0m     ws353 \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 10\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwordsim_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43msimilarity_type\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_goldstandard.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m: \n\u001B[1;32m     11\u001B[0m         a,b,val \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msplit()\n\u001B[1;32m     12\u001B[0m         val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(val)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'wordsim_relatedness_goldstandard.txt'"
     ]
    }
   ],
   "source": [
    "# Code for computing correlation between W2V similarity, and human judgements\n",
    "\n",
    "import gensim.downloader\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "gn = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "for similarity_type in ['relatedness', 'similarity']:\n",
    "    ws353 = []\n",
    "    for x in open(f'wordsim_{similarity_type}_goldstandard.txt'): \n",
    "        a,b,val = x.split()\n",
    "        val = float(val)\n",
    "        ws353.append( (a,b,val))\n",
    "    # spearmanr returns 2 vallues: correlation and pval. pval should be close to zero\n",
    "    print (similarity_type + ':', spearmanr(vals, ys)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf71c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}